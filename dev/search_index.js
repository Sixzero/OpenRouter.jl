var documenterSearchIndex = {"docs":
[{"location":"#OpenRouter","page":"Home","title":"OpenRouter","text":"Documentation for OpenRouter.\n\n","category":"section"},{"location":"#OpenRouter.AIMessage-Tuple{OpenRouter.AbstractRequestSchema, Dict}","page":"Home","title":"OpenRouter.AIMessage","text":"AIMessage(schema::AbstractRequestSchema, result::Dict; endpoint=nothing, elapsed=-1.0)\n\nConstruct an AIMessage by extracting all fields from raw API result. If endpoint is provided, cost is calculated from token usage.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.AbstractLLMStream","page":"Home","title":"OpenRouter.AbstractLLMStream","text":"AbstractLLMStream\n\nAbstract type for LLM stream callbacks.\n\nMust have fields:\n\nout: Output stream (e.g., stdout or pipe)\nschema: Request schema determining API format\nchunks: List of received AbstractStreamChunk chunks\nverbose: Whether to print verbose information\nkwargs: Custom keyword arguments\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.AbstractRequestSchema","page":"Home","title":"OpenRouter.AbstractRequestSchema","text":"Abstract base type for request schemas.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.AbstractResponseSchema","page":"Home","title":"OpenRouter.AbstractResponseSchema","text":"Abstract base type for response schemas.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.AbstractSchema","page":"Home","title":"OpenRouter.AbstractSchema","text":"Abstract base type for all API schemas.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.AbstractStreamChunk","page":"Home","title":"OpenRouter.AbstractStreamChunk","text":"AbstractStreamChunk\n\nAbstract type for stream chunks.\n\nMust have fields:\n\nevent: The event name\ndata: The data chunk  \njson: The JSON object or nothing if chunk doesn't contain JSON\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.AnthropicSchema","page":"Home","title":"OpenRouter.AnthropicSchema","text":"Anthropic Claude-style request schema.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.ChatCompletionResponseSchema","page":"Home","title":"OpenRouter.ChatCompletionResponseSchema","text":"Standard OpenAI-compatible response schema.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.ChatCompletionSchema","page":"Home","title":"OpenRouter.ChatCompletionSchema","text":"Standard OpenAI-compatible chat completion request schema. This is the default schema used by most providers.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.GeminiSchema","page":"Home","title":"OpenRouter.GeminiSchema","text":"Google Gemini-style request schema.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.HttpStreamCallback","page":"Home","title":"OpenRouter.HttpStreamCallback","text":"HttpStreamCallback\n\nHTTP-based streaming callback that prints content to output stream. When streaming completes, builds response body from chunks as if it was a normal API response.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.HttpStreamHooks","page":"Home","title":"OpenRouter.HttpStreamHooks","text":"HttpStreamHooks\n\nA stream callback that combines token counting with customizable hooks for various events.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.ModelConfig","page":"Home","title":"OpenRouter.ModelConfig","text":"Configuration for model calls, including provider/model slug and call parameters.\n\nSupported Parameters by Schema\n\nChatCompletionSchema (OpenAI-compatible)\n\nCommon parameters supported by most OpenAI-compatible providers:\n\ntemperature::Float64: Sampling temperature (0.0-2.0, default varies by model)\nmax_tokens::Int: Maximum tokens to generate\ntop_p::Float64: Nucleus sampling threshold (0.0-1.0)\nfrequency_penalty::Float64: Penalize frequent tokens (-2.0 to 2.0)\npresence_penalty::Float64: Penalize present tokens (-2.0 to 2.0)\nstop::Union{String, Vector{String}}: Stop sequences\nn::Int: Number of completions to generate\nstream::Bool: Enable streaming (handled automatically by streamcallback)\nlogprobs::Bool: Include log probabilities\ntop_logprobs::Int: Number of top log probabilities to return\nseed::Int: Random seed for deterministic sampling\nresponse_format::Dict: Structured output format (e.g., Dict(\"type\" => \"json_object\"))\n\nAnthropicSchema\n\nAnthropic-specific parameters:\n\nmax_tokens::Int: Maximum tokens to generate (required, default 1000)\ntemperature::Float64: Sampling temperature (0.0-1.0)\ntop_p::Float64: Nucleus sampling (0.0-1.0)\ntop_k::Int: Top-k sampling\nstop_sequences::Vector{String}: Stop sequences\ncache::Symbol: Prompt caching mode (:system, :tools, :last, :all, :all_but_last)\nmetadata::Dict: Request metadata\n\nGeminiSchema\n\nGoogle Gemini-specific parameters:\n\ntemperature::Float64: Sampling temperature\ntop_p::Float64 / topP::Float64: Nucleus sampling\ntop_k::Int / topK::Int: Top-k sampling\nmax_output_tokens::Int / maxOutputTokens::Int: Maximum output tokens\npresence_penalty::Float64 / presencePenalty::Float64: Presence penalty\nfrequency_penalty::Float64 / frequencyPenalty::Float64: Frequency penalty\nresponse_mime_type::String / responseMimeType::String: Output MIME type\nresponse_schema::Dict / responseSchema::Dict: Output schema\nresponse_json_schema::Dict / responseJsonSchema::Dict: JSON schema\nstop_sequences::Vector{String} / stopSequences::Vector{String}: Stop sequences\nthinkingConfig::Dict: Thinking/reasoning configuration\nthinkingLevel::Int: Reasoning depth level\nthinkingBudget::Int: Token budget for reasoning\ninclude_thoughts::Bool: Include reasoning in response\ncandidateCount::Int: Number of response candidates\nseed::Int: Random seed\nresponseLogprobs::Bool: Include log probabilities\nlogprobs::Int: Number of log probabilities\n\nResponseSchema (OpenAI Response API)\n\nFor gpt-5 and o-series models:\n\nmax_completion_tokens::Int: Maximum tokens in completion\nreasoning_effort::String: Reasoning effort level (\"low\", \"medium\", \"high\")\ntemperature::Float64: Sampling temperature\ntop_p::Float64: Nucleus sampling\nmodalities::Vector{String}: Output modalities (e.g., [\"text\", \"audio\"])\naudio::Dict: Audio output configuration\n\nExamples\n\n# OpenAI-compatible config\nconfig = ModelConfig(\"openai:openai/gpt-5.1\"; \n    temperature=0.7, \n    max_tokens=1000,\n    top_p=0.9\n)\n\n# Anthropic with caching\nconfig = ModelConfig(\"anthropic:anthropic/claude-sonnet-4.5\";\n    max_tokens=2000,\n    temperature=0.8,\n    cache=:all\n)\n\n# Gemini with thinking\nconfig = ModelConfig(\"google-ai-studio:google/gemini-2.5-flash\";\n    temperature=0.7,\n    maxOutputTokens=2000,\n    thinkingConfig=Dict(\n        :thinkingLevel => 2,\n        :include_thoughts => true\n    )\n)\n\n# Modify config later\nconfig.kwargs = merge(config.kwargs, (temperature=0.9,))\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.ModelConfig-Tuple{String}","page":"Home","title":"OpenRouter.ModelConfig","text":"ModelConfig(slug::String; schema=nothing, kwargs...)\n\nCreate a ModelConfig with the given slug and optional parameters.\n\nExample\n\nconfig = ModelConfig(\"openai:openai/gpt-5.1\"; temperature=0.7, max_tokens=1000)\nresponse = aigen(\"Hello\", config)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.ResponseSchema","page":"Home","title":"OpenRouter.ResponseSchema","text":"ResponseSchema <: AbstractRequestSchema\n\nSchema for OpenAI's Responses API (v1/responses endpoint). Used by newer models like gpt-5.1 and o-series models.\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.RunInfo","page":"Home","title":"OpenRouter.RunInfo","text":"RunInfo(; creation_time=time(), inference_start=nothing, last_message_time=nothing, stop_sequence=nothing)\n\nTracks run statistics and metadata during the streaming process.\n\nFields\n\ncreation_time: When the callback was created\ninference_start: When the model started processing\nlast_message_time: Timestamp of the last received message\nstop_sequence: The sequence that caused the generation to stop (if any). For OpenAI this can be:\nA specific stop sequence provided in the chunk's delta.stop_sequence\n\"stop\" if finish_reason is \"stop\"\nFor Anthropic this is the stop_sequence provided in the chunk.\n\nTiming Methods\n\nget_total_elapsed(info): Get total elapsed time since callback creation\nget_inference_elapsed(info): Get elapsed time for inference phase only\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.StreamChunk","page":"Home","title":"OpenRouter.StreamChunk","text":"StreamChunk\n\nA chunk of streaming data. A message is composed of multiple chunks.\n\nFields\n\nevent: The event name\ndata: The data chunk\njson: The JSON object or nothing if chunk doesn't contain JSON\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.TokenCounts","page":"Home","title":"OpenRouter.TokenCounts","text":"TokenCounts\n\nUniversal token counting struct. Fields are NON-OVERLAPPING for correct cost calculation.\n\nFields (non-overlapping)\n\nprompt_tokens::Int: Cache misses - input tokens NOT served from cache (charged at full price)\ninput_cache_read::Int: Cache hits - input tokens served from cache (charged at cache price)\ncompletion_tokens::Int: Output tokens\ntotal_tokens::Int: Sum of all input + output tokens\ninput_cache_write::Int: Tokens written to cache (Anthropic)\ninternal_reasoning::Int: Reasoning/thinking tokens (Gemini, DeepSeek R1)\ninput_audio_cache::Int: Audio tokens cached\n\nCost calculation\n\nTotal input = prompttokens + inputcacheread (no double counting) Cost = prompttokens × fullprice + inputcacheread × cacheprice\n\n\n\n\n\n","category":"type"},{"location":"#OpenRouter.GeminiConfig-Tuple{String}","page":"Home","title":"OpenRouter.GeminiConfig","text":"GeminiConfig(model_id::String; kwargs...)\n\nConvenience constructor for Gemini models with common parameter validation.\n\nGemini-specific Parameters\n\ntemperature::Float64: Sampling temperature (0.0-2.0)\ntopP::Float64: Nucleus sampling (0.0-1.0)\ntopK::Int: Top-k sampling\nmaxOutputTokens::Int: Maximum output tokens\nthinkingConfig::NamedTuple: Thinking configuration with fields:\nthinkingLevel::String: Reasoning depth (\"low\" or \"high\") - only for Pro models\nthinkingBudget::Int: Token budget for reasoning - for non-Pro models\ninclude_thoughts::Bool: Include reasoning in response\n\nExamples\n\n# Basic Gemini config\nconfig = GeminiConfig(\"google/gemini-2.5-flash\"; \n    temperature=0.7,\n    maxOutputTokens=2000\n)\n\n# Pro model with thinking level\nconfig = GeminiConfig(\"google/gemini-2.5-pro\";\n    temperature=0.8,\n    maxOutputTokens=3000,\n    thinkingConfig=(\n        thinkingLevel=\"high\",\n        include_thoughts=true\n    )\n)\n\n# Non-Pro model with thinking budget\nconfig = GeminiConfig(\"google/gemini-2.5-flash-thinking\";\n    thinkingConfig=(\n        thinkingBudget=1000,\n        include_thoughts=true\n    )\n)\n\nresponse = aigen(\"Explain quantum entanglement\", config)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter._aigen_core-Tuple{Any, OpenRouter.ProviderInfo, AbstractString, ProviderEndpoint}","page":"Home","title":"OpenRouter._aigen_core","text":"Core function that handles both streaming and non-streaming API calls.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter._list_models_unfiltered","page":"Home","title":"OpenRouter._list_models_unfiltered","text":"_list_models_unfiltered(api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::Vector{OpenRouterModel}\n\nInternal helper: return all models without any provider-based filtering, using the raw API. Used by update_db to avoid recursive use of the cache.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.acc_tokens-Tuple{OpenRouter.AbstractRequestSchema, OpenRouter.TokenCounts, OpenRouter.TokenCounts}","page":"Home","title":"OpenRouter.acc_tokens","text":"Accumulate tokens according to schema-specific logic.\n\nSchema-specific behavior\n\nAnthropicSchema: Replaces values (Anthropic sends cumulative counts)\nOther schemas: Adds values (most providers send deltas)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.add_custom_model","page":"Home","title":"OpenRouter.add_custom_model","text":"add_custom_model(id::String, name::String, description::String=\"Custom model\",\n                context_length::Union{Int,Nothing}=nothing,\n                pricing::Union{Pricing,Nothing}=nothing,\n                architecture::Union{Architecture,Nothing}=nothing)\n\nAdd a custom model to the local cache.\n\nExample\n\nadd_custom_model(\"echo/100tps\", \"Echo 100 TPS\", \"Fast echo model for testing\", 8192)\nadd_custom_model(\"local/llama3\", \"Local Llama 3\", \"Self-hosted Llama 3\", 4096)\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.add_model","page":"Home","title":"OpenRouter.add_model","text":"add_model(id::String, name::String, description::String=\"Custom model\",\n         context_length::Union{Int,Nothing}=nothing,\n         pricing::Union{Pricing,Nothing}=nothing,\n         architecture::Union{Architecture,Nothing}=nothing)\n\nAdd a model to the local cache.\n\nExample\n\nadd_model(\"echo/100tps\", \"Echo 100 TPS\", \"Fast echo model for testing\", 8192)\nadd_model(\"ollama/llama3\", \"Local Llama 3\", \"Self-hosted Llama 3\", 4096)\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.add_provider","page":"Home","title":"OpenRouter.add_provider","text":"add_provider(name::String, base_url::String, auth_header_format::String=\"Bearer\", \n            api_key_env_var::Union{String,Nothing}=nothing, \n            default_headers::Dict{String,String}=Dict{String,String}(),\n            model_name_transform::Union{Function,Nothing}=nothing,\n            schema::AbstractRequestSchema=ChatCompletionSchema(),\n            notes::String=\"Custom provider\")\n\nAdd a provider to the registry.\n\nExample\n\nadd_provider(\"echo\", \"http://localhost:8080/v1\", \"Bearer\", \"ECHO_API_KEY\")\nadd_provider(\"ollama\", \"http://localhost:11434/v1\", \"Bearer\")\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.aigen-Tuple{Any, String}","page":"Home","title":"OpenRouter.aigen","text":"aigen(prompt, provider_model::String; ...)\naigen(prompt, config::ModelConfig; ...)\n\nGenerate text using a specific provider and model, or a ModelConfig.\n\nArguments\n\nprompt: The input prompt (String or Vector of message dicts)\nprovider_model::String: Format \"Provider:model/slug\" (e.g., \"Together:moonshotai/kimi-k2-thinking\")\nconfig::ModelConfig: Model configuration with slug and parameters\n\nKeyword Arguments\n\nschema::Union{AbstractRequestSchema, Nothing}: Request schema to use (auto-detected if not provided)\napi_key::Union{String, Nothing}: Provider-specific API key (auto-detected from env if not provided)\nsys_msg: System message/instruction\nstreamcallback::Union{Nothing, AbstractLLMStream}: Stream callback for real-time processing\nkwargs...: Additional API parameters\n\nReturns\n\nAIMessage: Generated response with metadata (cost, tokens, etc.)\n\nExample\n\n# Using string slug\nresponse = aigen(\"Write a haiku about Julia programming\", \"Together:moonshotai/kimi-k2-thinking\")\n\n# Using ModelConfig\nconfig = ModelConfig(\"openai:openai/gpt-5.1\"; temperature=0.7, max_tokens=1000)\nresponse = aigen(\"Hello\", config)\n\n# Using system message\nresponse = aigen(\"Hello\", \"Anthropic:claude-3-sonnet\"; sys_msg=\"You are a helpful assistant\")\n\n# Using streaming\nusing OpenRouter\ncallback = HttpStreamCallback(; out=stdout)\nresponse = aigen(\"Count to 10\", \"anthropic:anthropic/claude-haiku-4.5\"; streamcallback=callback)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.aigen-Tuple{}","page":"Home","title":"OpenRouter.aigen","text":"aigen(; prompt, model, kwargs...)\n\nKwarg-only version of aigen. ```\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.aigen_raw-Tuple{Any, String}","page":"Home","title":"OpenRouter.aigen_raw","text":"aigen_raw(prompt, provider_model::String; \n          schema::Union{AbstractRequestSchema, Nothing} = nothing,\n          api_key::Union{String, Nothing} = nothing,\n          sys_msg = nothing,\n          streamcallback::Union{Nothing, AbstractLLMStream} = nothing,\n          kwargs...)\n\nGenerate text using a specific provider and model, returning raw API response and parsing components.\n\nThis function is useful for:\n\nTesting equivalence between streaming and non-streaming responses\nDebugging API response formats\nCustom response processing\n\nReturns\n\nNamedTuple: Contains (result, schema, provider_info, model_id, provider_endpoint, elapsed)\n\nExample\n\n# Compare streaming vs non-streaming raw responses\nraw_stream = aigen_raw(\"Hello\", \"anthropic:claude-3-sonnet\"; streamcallback=HttpStreamCallback())\nraw_normal = aigen_raw(\"Hello\", \"anthropic:claude-3-sonnet\")\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.anthropic_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.anthropic_model_transform","text":"anthropic_model_transform(model_id::String)::String\n\nTransform model IDs for Anthropic. Removes anthropic/ prefix and replaces dots with dashes. Also handles special cases and version matching.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_headers-Tuple{OpenRouter.ProviderInfo, AbstractString}","page":"Home","title":"OpenRouter.build_headers","text":"Build complete headers for a provider request.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_messages-Tuple{OpenRouter.AnthropicSchema, Any, Any}","page":"Home","title":"OpenRouter.build_messages","text":"Build messages array for AnthropicSchema.\n\nReturns a tuple: (messages, system_content)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_messages-Tuple{OpenRouter.ChatCompletionSchema, Any, Any}","page":"Home","title":"OpenRouter.build_messages","text":"Build messages array for ChatCompletionSchema.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_messages-Tuple{OpenRouter.GeminiSchema, Any, Any}","page":"Home","title":"OpenRouter.build_messages","text":"Build contents array for GeminiSchema.\n\nReturns a tuple: (contents, system_instruction)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_native_models_url-Tuple{OpenRouter.ProviderInfo}","page":"Home","title":"OpenRouter.build_native_models_url","text":"build_native_models_url(provider_info::ProviderInfo)::String\n\nBuild the models endpoint URL for a provider's native API.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_payload","page":"Home","title":"OpenRouter.build_payload","text":"Build the request payload for GeminiSchema.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.build_payload-2","page":"Home","title":"OpenRouter.build_payload","text":"Build the request payload for ChatCompletionSchema.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.build_payload-3","page":"Home","title":"OpenRouter.build_payload","text":"Build the request payload for AnthropicSchema.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.build_response_body-Tuple{OpenRouter.AnthropicSchema, OpenRouter.AbstractLLMStream}","page":"Home","title":"OpenRouter.build_response_body","text":"build_response_body(schema::AnthropicSchema, cb::AbstractLLMStream; verbose::Bool = false, kwargs...)\n\nBuild response body from chunks to mimic standard Anthropic API response.\n\nNote: Limited functionality. Does NOT support tool use.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_response_body-Tuple{OpenRouter.ChatCompletionSchema, OpenRouter.AbstractLLMStream}","page":"Home","title":"OpenRouter.build_response_body","text":"build_response_body(schema::ChatCompletionSchema, cb::AbstractLLMStream; verbose::Bool = false, kwargs...)\n\nBuild response body from chunks to mimic standard ChatCompletion API response.\n\nNote: Limited functionality. Does NOT support tool use, refusals, logprobs.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_response_body-Tuple{OpenRouter.GeminiSchema, OpenRouter.AbstractLLMStream}","page":"Home","title":"OpenRouter.build_response_body","text":"build_response_body(schema::GeminiSchema, cb::AbstractLLMStream; verbose::Bool = false, kwargs...)\n\nBuild response body from chunks to mimic standard Gemini API response.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_response_body-Tuple{OpenRouter.ResponseSchema, OpenRouter.AbstractLLMStream}","page":"Home","title":"OpenRouter.build_response_body","text":"build_response_body(schema::ResponseSchema, cb::AbstractLLMStream; verbose::Bool = false, kwargs...)\n\nBuild response body from chunks.  Optimized to find the final response.completed object immediately (O(1) effectively), with a fallback reconstruction for interrupted streams.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.build_url","page":"Home","title":"OpenRouter.build_url","text":"Build the URL for ChatCompletionSchema.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.build_url-2","page":"Home","title":"OpenRouter.build_url","text":"Build the URL for AnthropicSchema.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.build_url-3","page":"Home","title":"OpenRouter.build_url","text":"Build the URL for GeminiSchema (handles model parameter substitution and streaming).\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.calculate_cost","page":"Home","title":"OpenRouter.calculate_cost","text":"Calculate cost for a given endpoint and token usage. Unwraps .pricing. Warns if cost cannot be determined (e.g. missing pricing).\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.calculate_cost-Tuple{Pricing, Union{Nothing, OpenRouter.TokenCounts, Dict}}","page":"Home","title":"OpenRouter.calculate_cost","text":"Calculate cost based on pricing and token usage.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.calculate_cost-Tuple{ProviderEndpoint, Union{Nothing, Dict}}","page":"Home","title":"OpenRouter.calculate_cost","text":"Calculate cost for a given endpoint and token usage. Unwraps .pricing. Warns if cost cannot be determined (e.g. missing pricing).\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.callback-Tuple{HttpStreamCallback, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.callback","text":"callback(cb::AbstractLLMStream, chunk::AbstractStreamChunk; kwargs...)\n\nProcess chunk and print it. Wrapper for:\n\nextract content from chunk using extract_content\nprint content to output stream using print_content\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.callback-Tuple{OpenRouter.AbstractLLMStream, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.callback","text":"callback(cb::AbstractLLMStream, chunk::AbstractStreamChunk; kwargs...)\n\nProcess chunk and print it. Wrapper for:\n\nextract content from chunk using extract_content\nprint content to output stream using print_content\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.cerebras_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.cerebras_model_transform","text":"cerebras_model_transform(model_id::String)::String\n\nTransform model IDs for Cerebras.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.cohere_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.cohere_model_transform","text":"cohere_model_transform(model_id::String)::String\n\nTransform model IDs for Cohere. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.configure_stream_callback!-Tuple{OpenRouter.AbstractLLMStream, OpenRouter.AbstractRequestSchema, OpenRouter.ProviderInfo, ProviderEndpoint}","page":"Home","title":"OpenRouter.configure_stream_callback!","text":"configure_stream_callback!(cb::AbstractLLMStream, schema::AbstractRequestSchema, provider_info::ProviderInfo, provider_endpoint::ProviderEndpoint)\n\nConfigure stream callback with schema and provider information. For HttpStreamHooks, also sets up pricing for accurate cost calculation.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.create_stub_endpoint-Tuple{AbstractString, AbstractString}","page":"Home","title":"OpenRouter.create_stub_endpoint","text":"create_stub_endpoint(provider_name, model_id; pricing=nothing)\n\nCreate a stub ProviderEndpoint for local/non-OpenRouter providers.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.create_stub_endpoint_zero_pricing-Tuple{Any, Any}","page":"Home","title":"OpenRouter.create_stub_endpoint_zero_pricing","text":"Create stub with zero pricing (for Ollama etc. where we want cost tracking).\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.deepseek_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.deepseek_model_transform","text":"deepseek_model_transform(model_id::String)::String\n\nTransform model IDs for DeepSeek. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.echo_handler-Tuple{HTTP.Messages.Request}","page":"Home","title":"OpenRouter.echo_handler","text":"Route request to appropriate response based on endpoint.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_chunks-Tuple{OpenRouter.AbstractRequestSchema, AbstractString}","page":"Home","title":"OpenRouter.extract_chunks","text":"extract_chunks(schema::AbstractRequestSchema, blob::AbstractString;\n    spillover::AbstractString = \"\", verbose::Bool = false, kwargs...)\n\nExtract chunks from received SSE blob. Correctly implements SSE spec field parsing.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_config-Tuple{ModelConfig, Any}","page":"Home","title":"OpenRouter.extract_config","text":"Extract slug and merge kwargs from ModelConfig with call-time kwargs. Call-time kwargs take precedence over config kwargs.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_content-Tuple{OpenRouter.AnthropicSchema, Dict}","page":"Home","title":"OpenRouter.extract_content","text":"Extract response content for AnthropicSchema.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_content-Tuple{OpenRouter.AnthropicSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.extract_content","text":"extract_content(schema::AnthropicSchema, chunk::AbstractStreamChunk;\n    include_thinking::Bool = true, kwargs...)\n\nExtract content from Anthropic chunk.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_content-Tuple{OpenRouter.ChatCompletionSchema, Dict}","page":"Home","title":"OpenRouter.extract_content","text":"Extract response content for ChatCompletionSchema.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_content-Tuple{OpenRouter.ChatCompletionSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.extract_content","text":"extract_content(schema::ChatCompletionSchema, chunk::AbstractStreamChunk; kwargs...)\n\nExtract content from ChatCompletion chunk.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_content-Tuple{OpenRouter.GeminiSchema, Dict}","page":"Home","title":"OpenRouter.extract_content","text":"Extract response content for GeminiSchema.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_content-Tuple{OpenRouter.GeminiSchema, StreamChunk}","page":"Home","title":"OpenRouter.extract_content","text":"extract_content(schema::GeminiSchema, chunk::StreamChunk; kwargs...)\n\nExtract regular (non-reasoning) content from Gemini chunk.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_content-Tuple{OpenRouter.ResponseSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.extract_content","text":"extract_content(schema::ResponseSchema, chunk::AbstractStreamChunk; kwargs...)\n\nExtract content from Response API chunk.  Only extracts 'delta' to ensure stream consumers don't print duplicate content  (since 'done' events contain the full text).\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_finish_reason-Tuple{OpenRouter.ChatCompletionSchema, Dict}","page":"Home","title":"OpenRouter.extract_finish_reason","text":"Extract finish reason from API response based on schema.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_provider_from_model-Tuple{String}","page":"Home","title":"OpenRouter.extract_provider_from_model","text":"extract_provider_from_model(model_name::String) -> String\n\nExtract provider name from model name in format \"provider:author/model_id\" or fallback to \"openai\".\n\nExamples\n\nextract_provider_from_model(\"openai:openai/gpt-4\") # => \"openai\"\nextract_provider_from_model(\"anthropic:anthropic/claude-3-5-sonnet\") # => \"anthropic\"\nextract_provider_from_model(\"cerebras:meta-llama/llama-3.1-8b\") # => \"cerebras\"\nextract_provider_from_model(\"gpt-4\") # => \"openai\" (fallback)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_reasoning-Tuple{OpenRouter.ChatCompletionSchema, Dict}","page":"Home","title":"OpenRouter.extract_reasoning","text":"Extract reasoning content from API response based on schema. Returns nothing if schema doesn't support reasoning or no reasoning found.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_reasoning_from_chunk-Tuple{OpenRouter.GeminiSchema, StreamChunk}","page":"Home","title":"OpenRouter.extract_reasoning_from_chunk","text":"extract_reasoning_from_chunk(schema::GeminiSchema, chunk::StreamChunk)\n\nExtract reasoning/thinking content from Gemini chunk (parts with \"thought\": true).\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_response-Tuple{OpenRouter.ChatCompletionResponseSchema, Dict}","page":"Home","title":"OpenRouter.extract_response","text":"Extract full response for ChatCompletionResponseSchema.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_tokens-Tuple{OpenRouter.ChatCompletionSchema, Union{Dict, JSON3.Object}}","page":"Home","title":"OpenRouter.extract_tokens","text":"Extract token usage information from API response based on schema. Returns TokenCounts struct with standardized field names.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.extract_tool_calls-Tuple{OpenRouter.ChatCompletionSchema, Dict}","page":"Home","title":"OpenRouter.extract_tool_calls","text":"Extract tool calls from API response based on schema. Returns nothing if no tool calls found.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.fetch_native_models-Tuple{OpenRouter.ProviderInfo, String}","page":"Home","title":"OpenRouter.fetch_native_models","text":"fetch_native_models(provider_info::ProviderInfo, api_key::String)::Vector{Dict}\n\nFetch models directly from a provider's native API. Returns raw model data as returned by the provider.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.fireworks_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.fireworks_model_transform","text":"fireworks_model_transform(model_id::String)::String\n\nTransform model IDs for Fireworks. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_inference_elapsed-Tuple{OpenRouter.RunInfo}","page":"Home","title":"OpenRouter.get_inference_elapsed","text":"get_inference_elapsed(info::RunInfo)\n\nGet elapsed time for inference (time between first inference and last message). Returns time in seconds or nothing if inference hasn't started.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_provider_auth_header-Tuple{AbstractString, AbstractString}","page":"Home","title":"OpenRouter.get_provider_auth_header","text":"Build an auth header pair (name => value) for a provider + API key, or nothing if provider is unknown.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_provider_auth_header-Tuple{OpenRouter.ProviderInfo, AbstractString}","page":"Home","title":"OpenRouter.get_provider_auth_header","text":"Build an auth header pair (name => value) for a ProviderInfo + API key.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_provider_base_url-Tuple{AbstractString}","page":"Home","title":"OpenRouter.get_provider_base_url","text":"Get just the base URL for a provider slug, or nothing if unknown.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_provider_env_var_name-Tuple{AbstractString}","page":"Home","title":"OpenRouter.get_provider_env_var_name","text":"Return the configured API key env var name for a provider, or nothing.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_provider_info-Tuple{AbstractString}","page":"Home","title":"OpenRouter.get_provider_info","text":"Get the provider info for a given slug, or nothing if unknown.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_provider_schema-Tuple{OpenRouter.ProviderInfo, AbstractString}","page":"Home","title":"OpenRouter.get_provider_schema","text":"Get the appropriate schema for a provider info and model. For OpenAI, use ResponseSchema for gpt-5 and o-series models.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.get_total_elapsed-Tuple{OpenRouter.RunInfo}","page":"Home","title":"OpenRouter.get_total_elapsed","text":"get_total_elapsed(info::RunInfo)\n\nGet total elapsed time since callback creation. Returns time in seconds or nothing if no messages received.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.google_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.google_model_transform","text":"google_model_transform(model_id::String)::String\n\nTransform model IDs for Google. Removes google/ prefix.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.groq_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.groq_model_transform","text":"groq_model_transform(model_id::String)::String\n\nTransform OpenRouter model IDs to Groq-specific model IDs. Handles various model mappings for Groq's native API.\n\nExamples\n\ngroq_model_transform(\"moonshotai/kimi-k2-0905\")  # => \"moonshotai/kimi-k2-instruct-0905\"\ngroq_model_transform(\"other/model\")              # => \"other/model\"\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.handle_error_message-Tuple{OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.handle_error_message","text":"handle_error_message(chunk::AbstractStreamChunk; kwargs...)\n\nHandle error messages from streaming response. Always throws on error.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.is_done-Tuple{OpenRouter.ChatCompletionSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.is_done","text":"is_done(schema::ChatCompletionSchema, chunk::AbstractStreamChunk; kwargs...)\n\nCheck if streaming is done for ChatCompletion format. Checks for finish_reason in choices or [DONE] marker.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.is_done-Tuple{OpenRouter.ResponseSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.is_done","text":"is_done(schema::ResponseSchema, chunk::AbstractStreamChunk; kwargs...)\n\nCheck if streaming is done for Response API format.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.is_known_provider-Tuple{AbstractString}","page":"Home","title":"OpenRouter.is_known_provider","text":"Check if this provider slug is known.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.is_start-Tuple{OpenRouter.AnthropicSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.is_start","text":"is_start(schema::AnthropicSchema, chunk::AbstractStreamChunk; kwargs...)\n\nCheck if streaming has started for Anthropic format.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.is_start-Tuple{OpenRouter.ChatCompletionSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.is_start","text":"is_start(schema::ChatCompletionSchema, chunk::AbstractStreamChunk; kwargs...)\n\nCheck if streaming has started for ChatCompletion format.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.is_start-Tuple{OpenRouter.GeminiSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.is_start","text":"is_start(schema::GeminiSchema, chunk::AbstractStreamChunk; kwargs...)\n\nCheck if streaming has started for Gemini format. Gemini doesn't have explicit start events, so we check for first content with reasoning (thought=true).\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.is_start-Tuple{OpenRouter.ResponseSchema, OpenRouter.AbstractStreamChunk}","page":"Home","title":"OpenRouter.is_start","text":"is_start(schema::ResponseSchema, chunk::AbstractStreamChunk; kwargs...)\n\nCheck if streaming has started for Response API format.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.list_aliases-Tuple{}","page":"Home","title":"OpenRouter.list_aliases","text":"list_aliases()::Dict{String, String}\n\nList all available model aliases.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.list_config_parameters-Tuple{ModelConfig}","page":"Home","title":"OpenRouter.list_config_parameters","text":"list_config_parameters(config::ModelConfig)\n\nList parameters currently set in a ModelConfig.\n\nExample\n\nconfig = ModelConfig(\"openai:openai/gpt-5.1\"; temperature=0.7, max_tokens=1000)\nlist_config_parameters(config)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.list_embeddings_models","page":"Home","title":"OpenRouter.list_embeddings_models","text":"list_embeddings_models(api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::Vector{OpenRouterEmbeddingModel}\n\nReturn parsed embedding models list as Julia structs. Uses OPENROUTERAPIKEY environment variable by default.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_embeddings_models_raw","page":"Home","title":"OpenRouter.list_embeddings_models_raw","text":"list_embeddings_models_raw(api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::String\n\nReturn raw JSON string of embedding models list. Uses OPENROUTERAPIKEY environment variable by default.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_endpoints","page":"Home","title":"OpenRouter.list_endpoints","text":"list_endpoints(model_id::String, api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::ModelProviders\n\nReturn parsed endpoints for a specific model from OpenRouter as Julia struct. Model ID should be in format \"author/slug\" (e.g., \"moonshotai/kimi-k2-thinking\"). Uses OPENROUTERAPIKEY environment variable by default.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_endpoints_raw","page":"Home","title":"OpenRouter.list_endpoints_raw","text":"list_endpoints_raw(model_id::String, api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::String\n\nReturn raw JSON string of endpoints for a specific model from OpenRouter. Model ID should be in format \"author/slug\" (e.g., \"moonshotai/kimi-k2-thinking\"). Uses OPENROUTERAPIKEY environment variable by default.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_known_providers-Tuple{}","page":"Home","title":"OpenRouter.list_known_providers","text":"List all known provider slugs.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.list_models","page":"Home","title":"OpenRouter.list_models","text":"list_models(provider_filter::Union{String, Nothing} = nothing, api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::Vector{OpenRouterModel}\n\nReturn parsed model list as Julia structs, optionally filtered by provider. Uses OPENROUTERAPIKEY environment variable by default.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_models_raw","page":"Home","title":"OpenRouter.list_models_raw","text":"list_models_raw(api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::String\n\nReturn raw JSON string of model list. Uses OPENROUTERAPIKEY environment variable by default.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_native_models","page":"Home","title":"OpenRouter.list_native_models","text":"list_native_models(provider_slug::String, api_key::String = \"\")::Vector{Dict}\n\nList models using a provider's native API. Returns raw model data as returned by the provider.\n\nExample\n\nmodels = list_native_models(\"cerebras\")\nmodels = list_native_models(\"openai\", \"your-api-key\")\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_provider_endpoints","page":"Home","title":"OpenRouter.list_provider_endpoints","text":"list_provider_endpoints(provider_filter::String, api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::Vector{ProviderEndpoint}\n\nReturn all ProviderEndpoint entries hosted by the given provider.\n\nThis uses the cached model database with endpoints; it will fetch endpoints as needed on first call.\n\nExample:\n\ngroq_eps = list_provider_endpoints(\"groq\")\nfor ep in groq_eps\n    println(ep.provider_name, \" \", ep.name, \" (\", ep.model_name, \")\")\nend\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_providers","page":"Home","title":"OpenRouter.list_providers","text":"list_providers(model_id::String, api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::ModelProviders\n\nReturn parsed providers for a specific model as Julia struct. Model ID should be in format \"author/slug\" (e.g., \"moonshotai/kimi-k2-thinking\"). Uses OPENROUTERAPIKEY environment variable by default.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_providers-Tuple{}","page":"Home","title":"OpenRouter.list_providers","text":"list_providers()\n\nList all registered providers.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.list_providers_raw","page":"Home","title":"OpenRouter.list_providers_raw","text":"list_providers_raw(model_id::String, api_key::String = get(ENV, \"OPENROUTER_API_KEY\", \"\"))::String\n\nReturn raw JSON string of providers for a specific model. ...\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.list_schema_parameters-Tuple{Type{OpenRouter.ChatCompletionSchema}}","page":"Home","title":"OpenRouter.list_schema_parameters","text":"list_schema_parameters(schema::Type{<:AbstractRequestSchema})\nlist_schema_parameters(schema::AbstractRequestSchema)\n\nList common parameters supported by a schema type.\n\nExample\n\nlist_schema_parameters(ChatCompletionSchema)\nlist_schema_parameters(AnthropicSchema)\nlist_schema_parameters(GeminiSchema)\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.minimax_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.minimax_model_transform","text":"minimax_model_transform(model_id::String)::String\n\nTransform model IDs for Minimax. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.mistral_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.mistral_model_transform","text":"mistral_model_transform(model_id::String)::String\n\nTransform model IDs for Mistral. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.moonshotai_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.moonshotai_model_transform","text":"moonshotai_model_transform(model_id::String)::String\n\nTransform model IDs for MoonshotAI. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.needs_tool_execution-Tuple{OpenRouter.RunInfo}","page":"Home","title":"OpenRouter.needs_tool_execution","text":"needs_tool_execution(info::RunInfo)\n\nCheck if the run was terminated because the model is requested tool execution (with stop_sequence).\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.normalize_messages-Tuple{Any, Any}","page":"Home","title":"OpenRouter.normalize_messages","text":"Normalize prompt + sys_msg into a flat vector of AbstractMessage.\n\nAccepted prompt forms:\n\nString                => one UserMessage\nAbstractMessage       => wrapped in a vector\nVector of items       => each element may be\nAbstractMessage\nString            => treated as UserMessage\nanything else     => treated as UserMessage with that content\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.ollama_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.ollama_model_transform","text":"ollama_model_transform(model_id::String)::String\n\nTransform model IDs for Ollama. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.openai_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.openai_model_transform","text":"openai_model_transform(model_id::String)::String\n\nTransform model IDs for OpenAI. Removes openai/ prefix and handles specific mappings.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.parse_embedding_models-Tuple{String}","page":"Home","title":"OpenRouter.parse_embedding_models","text":"parse_embedding_models(json_str::String)::Vector{OpenRouterEmbeddingModel}\n\nParse OpenRouter embedding models JSON response into Julia structs.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.parse_endpoints-Tuple{String}","page":"Home","title":"OpenRouter.parse_endpoints","text":"parse_endpoints(json_str::String)::ModelProviders\n\nParse model providers JSON response into Julia struct.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.parse_models-Tuple{String}","page":"Home","title":"OpenRouter.parse_models","text":"parse_models(json_str::String)::Vector{OpenRouterModel}\n\nParse OpenRouter models JSON response into Julia structs.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.print_content-Tuple{Channel, AbstractString}","page":"Home","title":"OpenRouter.print_content","text":"print_content(out::Channel, text::AbstractString; kwargs...)\n\nPrint content to Channel.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.print_content-Tuple{IO, AbstractString}","page":"Home","title":"OpenRouter.print_content","text":"print_content(out::IO, text::AbstractString; kwargs...)\n\nPrint content to IO output stream.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.print_content-Tuple{Nothing, AbstractString}","page":"Home","title":"OpenRouter.print_content","text":"print_content(out::Nothing, text::Any; kwargs...)\n\nDo nothing if output stream is nothing.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.remove_custom_model-Tuple{String}","page":"Home","title":"OpenRouter.remove_custom_model","text":"remove_custom_model(id::String)\n\nRemove a custom model from the local cache.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.remove_model-Tuple{String}","page":"Home","title":"OpenRouter.remove_model","text":"remove_model(id::String)\n\nRemove a model from the local cache.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.remove_provider-Tuple{String}","page":"Home","title":"OpenRouter.remove_provider","text":"remove_provider(name::String)\n\nRemove a provider from the registry.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.resolve_model_alias-Tuple{AbstractString}","page":"Home","title":"OpenRouter.resolve_model_alias","text":"resolve_model_alias(model_id::String)::String\n\nResolve a model alias to the full provider:model format. If the input is not an alias, returns it unchanged.\n\nExample\n\nresolve_model_alias(\"gemf\")  # Returns \"google-ai-studio:google/gemini-2.5-flash-preview-09-2025\"\nresolve_model_alias(\"anthropic:claude-3-sonnet\")  # Returns unchanged\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.sambanova_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.sambanova_model_transform","text":"sambanova_model_transform(model_id::String)::String\n\nTransform model IDs for SambaNova. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.streamed_request!-Tuple{OpenRouter.AbstractLLMStream, Any, Any, IOBuffer}","page":"Home","title":"OpenRouter.streamed_request!","text":"streamed_request!(cb::AbstractLLMStream, url, headers, input; kwargs...)\n\nEnd-to-end wrapper for POST streaming requests. Modifies callback object (cb.chunks) in-place and returns response object.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.strip_provider_prefix-Tuple{AbstractString, AbstractString}","page":"Home","title":"OpenRouter.strip_provider_prefix","text":"strip_provider_prefix(model_id::AbstractString, provider::AbstractString)::AbstractString\n\nRemove provider prefix from model ID if present. Helper function for model transformations.\n\nExamples\n\nstrip_provider_prefix(\"openai/gpt-4\", \"openai\")     # => \"gpt-4\"\nstrip_provider_prefix(\"gpt-4\", \"openai\")            # => \"gpt-4\"\nstrip_provider_prefix(\"google/gemini-pro\", \"google\") # => \"gemini-pro\"\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.swap_echo_port!-Tuple{Any, Any}","page":"Home","title":"OpenRouter.swap_echo_port!","text":"Swap echo provider ports from from to to.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.together_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.together_model_transform","text":"together_model_transform(model_id::String)::String\n\nTransform model IDs for Together. Currently returns unchanged.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.transform_model_name-Tuple{OpenRouter.ProviderInfo, AbstractString}","page":"Home","title":"OpenRouter.transform_model_name","text":"Transform model name according to provider-specific rules.\n\n\n\n\n\n","category":"method"},{"location":"#OpenRouter.with_echo_server","page":"Home","title":"OpenRouter.with_echo_server","text":"Run function with echo server on given port.\n\n\n\n\n\n","category":"function"},{"location":"#OpenRouter.xai_model_transform-Tuple{AbstractString}","page":"Home","title":"OpenRouter.xai_model_transform","text":"xai_model_transform(model_id::String)::String\n\nTransform model IDs for xAI. Removes x-ai/ prefix and handles specific mappings.\n\n\n\n\n\n","category":"method"}]
}
